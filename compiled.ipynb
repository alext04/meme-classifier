{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precog Recruitment Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies (sourced cited at the bottom where required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id=input()\n",
    "img_path = f\"data/img/{img_id}.png\"\n",
    "# Check if the input image exists\n",
    "if not os.path.exists(img_path):\n",
    "    print(\"Input image not found.\")\n",
    "    raise(NameError)\n",
    "\n",
    "\n",
    "file_dir=\"curdir\"\n",
    "if os.path.exists(file_dir):\n",
    "    shutil.rmtree(file_dir)\n",
    "os.makedirs(file_dir)\n",
    "\n",
    "# Define the output image path\n",
    "output_image_path = os.path.join(file_dir, \"caption.jpg\")\n",
    "# Copy the input image to the output folder with the specified name\n",
    "shutil.copy(img_path, output_image_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption Removal (Inpainting) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_ocr\n",
    "# helper Function to calculate midpoint of a line\n",
    "def midpoint(x1, y1, x2, y2):\n",
    "    x_mid = int((x1 + x2) / 2)\n",
    "    y_mid = int((y1 + y2) / 2)\n",
    "    return (x_mid, y_mid)\n",
    "\n",
    "\n",
    "# Initialize keras-ocr pipeline\n",
    "pipeline = keras_ocr.pipeline.Pipeline()\n",
    "\n",
    "# Path to the image with text\n",
    "image_path = 'curdir/caption.jpg'\n",
    "\n",
    "# Read the image\n",
    "image = keras_ocr.tools.read(image_path) \n",
    "\n",
    "# Recognize text in the image\n",
    "predictions = pipeline.recognize([image])\n",
    "\n",
    "# Create a mask for inpainting\n",
    "mask = np.zeros(image.shape[:2], dtype=\"uint8\")\n",
    "\n",
    "# Iterate through predicted text regions and create mask\n",
    "for box in predictions[0]:\n",
    "    x0, y0 = box[1][0]\n",
    "    x1, y1 = box[1][1] \n",
    "    x2, y2 = box[1][2]\n",
    "    x3, y3 = box[1][3]\n",
    "    \n",
    "    # Calculate midpoints for line drawing\n",
    "    x_mid0, y_mid0 = midpoint(x1, y1, x2, y2)\n",
    "    x_mid1, y_mid1 = midpoint(x0, y0, x3, y3)\n",
    "    \n",
    "    # Calculate thickness based on line length\n",
    "    thickness = int(math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2))\n",
    "    \n",
    "    # Draw line on mask\n",
    "    cv2.line(mask, (x_mid0, y_mid0), (x_mid1, y_mid1), 255, thickness)\n",
    "\n",
    "# Inpaint the text regions\n",
    "inpainted_image = cv2.inpaint(image, mask, 7, cv2.INPAINT_NS)\n",
    "\n",
    "\n",
    "# Save the image without text\n",
    "cv2.imwrite('curdir/nocaption.jpg', cv2.cvtColor(inpainted_image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Extraction\n",
    "\n",
    "The text is extracted from the dataset as it is provided, but text extraction using OCR is implemented in the text_recognition for more general applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "\n",
    "file_paths = ['data/train.jsonl', 'data/test.jsonl','data/dev.jsonl'] \n",
    "\n",
    "given_text = None\n",
    "for file_path in file_paths:\n",
    "        with jsonlines.open(file_path) as reader:\n",
    "            for obj in reader:\n",
    "                # print(obj[\"id\"])\n",
    "                \n",
    "                if obj[\"id\"] == int(img_id):\n",
    "                    \n",
    "                    given_text=obj[\"text\"]\n",
    "\n",
    "print(given_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection\n",
    "Yolov8 from the ultralytics module was used as its pretrained model gave the highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO('yolov8m.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object Detection with Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "model(source=\"curdir/caption.jpg\", show=False, conf=0.5, save=True,project='curdir', name='captions')\n",
    "\n",
    "image = mpimg.imread(\"curdir/captions/caption.jpg\")\n",
    "\n",
    "    # Display the image\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object Detection with caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_noc = model(source=\"curdir/nocaption.jpg\", show=False, conf=0.5, save=True,project='curdir', name='no_captions')\n",
    "\n",
    "image = mpimg.imread(\"curdir/no_captions/nocaption.jpg\")\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race Detection\n",
    "using DeepFace module , in paticular the gender and race and emotion detector. The model also supports detection of age and emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "analysis = DeepFace.analyze(img_path, actions = [\"gender\", \"race\"],enforce_detection=False)\n",
    "print(analysis)\n",
    "prompt_caption_gen=\" \"\n",
    "for person in analysis:\n",
    "    prompt_caption_gen += person[\"dominant_race\"] + \" \" + person[\"dominant_gender\"] + \" \"\n",
    "    \n",
    "print(prompt_caption_gen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from promptcap import PromptCap\n",
    "\n",
    "model = PromptCap(\"tifa-benchmark/promptcap-coco-vqa\")  # also support OFA checkpoints. e.g. \"OFA-Sys/ofa-large\"\n",
    "\n",
    "image = \"curdir/nocaption.jpg\"\n",
    "caption_generated=model.caption(prompt_caption_gen, image)\n",
    "print(caption_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Similarity using semantic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "vector=model.encode([given_text,caption_generated])\n",
    "\n",
    "print(cosine_similarity([vector[0]],vector[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hateful Meme Classifier Based on text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "\n",
    "# file_paths = ['data/train.jsonl','data/dev.jsonl'] \n",
    "\n",
    "# given_label = None\n",
    "# for file_path in file_paths:\n",
    "#         with jsonlines.open(file_path) as reader:\n",
    "#             for obj in reader:\n",
    "#                 # print(obj[\"id\"])\n",
    "                \n",
    "#                 if obj[\"id\"] == int(img_id):\n",
    "                    \n",
    "#                     given_label=obj[\"text\"]\n",
    "                    \n",
    "# # if label is None it is in the test set\n",
    "tokens = tokenizer.encode(given_tex, return_tensors='pt')\n",
    "result = model(tokens)\n",
    "score= int(torch.argmax(result.logits))+1\n",
    "print(score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
