{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Classification System\n",
    "The goal of this classification system is to determine whether a given image is a meme or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking input for image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id=input()\n",
    "img_path = f\"../data/img/{img_id}.png\"\n",
    "# Check if the input image exists\n",
    "if not os.path.exists(img_path):\n",
    "    print(\"Input image not found.\")\n",
    "    raise(NameError)\n",
    "\n",
    "\n",
    "file_dir=\"curdir\"\n",
    "if os.path.exists(file_dir):\n",
    "    shutil.rmtree(file_dir)\n",
    "os.makedirs(file_dir)\n",
    "\n",
    "# Define the output image path\n",
    "output_image_path = os.path.join(file_dir, \"caption.jpg\")\n",
    "# Copy the input image to the output folder with the specified name\n",
    "shutil.copy(img_path, output_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_ocr\n",
    "# helper Function to calculate midpoint of a line\n",
    "def midpoint(x1, y1, x2, y2):\n",
    "    x_mid = int((x1 + x2) / 2)\n",
    "    y_mid = int((y1 + y2) / 2)\n",
    "    return (x_mid, y_mid)\n",
    "\n",
    "\n",
    "# Initialize keras-ocr pipeline\n",
    "pipeline = keras_ocr.pipeline.Pipeline()\n",
    "\n",
    "# Path to the image with text\n",
    "image_path = 'curdir/caption.jpg'\n",
    "\n",
    "# Read the image\n",
    "image = keras_ocr.tools.read(image_path) \n",
    "\n",
    "# Recognize text in the image\n",
    "predictions = pipeline.recognize([image])\n",
    "\n",
    "# Create a mask for inpainting\n",
    "mask = np.zeros(image.shape[:2], dtype=\"uint8\")\n",
    "\n",
    "# Iterate through predicted text regions and create mask\n",
    "for box in predictions[0]:\n",
    "    x0, y0 = box[1][0]\n",
    "    x1, y1 = box[1][1] \n",
    "    x2, y2 = box[1][2]\n",
    "    x3, y3 = box[1][3]\n",
    "    \n",
    "    # Calculate midpoints for line drawing\n",
    "    x_mid0, y_mid0 = midpoint(x1, y1, x2, y2)\n",
    "    x_mid1, y_mid1 = midpoint(x0, y0, x3, y3)\n",
    "    \n",
    "    # Calculate thickness based on line length\n",
    "    thickness = int(math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2))\n",
    "    \n",
    "    # Draw line on mask\n",
    "    cv2.line(mask, (x_mid0, y_mid0), (x_mid1, y_mid1), 255, thickness)\n",
    "\n",
    "# Inpaint the text regions\n",
    "inpainted_image = cv2.inpaint(image, mask, 7, cv2.INPAINT_NS)\n",
    "\n",
    "\n",
    "# Save the image without text\n",
    "cv2.imwrite('curdir/nocaption.jpg', cv2.cvtColor(inpainted_image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Extraction\n",
    "\n",
    "The text is extracted from the dataset as it is provided, but text extraction using OCR is implemented in the text_recognition for more general applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "\n",
    "file_paths = ['../data/train.jsonl', '../data/test.jsonl','../data/dev.jsonl'] \n",
    "\n",
    "given_text = None\n",
    "for file_path in file_paths:\n",
    "        with jsonlines.open(file_path) as reader:\n",
    "            for obj in reader:\n",
    "                # print(obj[\"id\"])\n",
    "                \n",
    "                if obj[\"id\"] == int(img_id):\n",
    "                    \n",
    "                    given_text=obj[\"text\"]\n",
    "                    \n",
    "\n",
    "print(given_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Object Detection without caption\n",
    "Yolov8 from the ultralytics module was used as its pretrained model gave the highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "model = YOLO('yolov8m.pt')\n",
    "\n",
    "predictions = model(source=\"curdir/nocaption.jpg\", show=False, conf=0.5, save=True,project='curdir', name='no_captions',save_txt=None)\n",
    "labels={0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
    "\n",
    "# print(predictions)\n",
    "# print(predictions[0])\n",
    "labels_seen=[]\n",
    "prompt_caption_gen=\" \"\n",
    "for idx, prediction in enumerate(predictions[0].boxes.xywhn): \n",
    "    cls = int(predictions[0].boxes.cls[idx].item())\n",
    "   \n",
    "    label=labels[cls]\n",
    "    \n",
    "    if cls!=0 and cls not in labels_seen:\n",
    "        prompt_caption_gen+=label+\" \"\n",
    "    labels_seen.append(cls)\n",
    "    \n",
    "          \n",
    "        \n",
    "          \n",
    "# print(prompt_caption_gen)   \n",
    "image = mpimg.imread(\"curdir/no_captions/nocaption.jpg\")\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race Detection\n",
    "using DeepFace module , in paticular the gender and race and emotion detector. The model also supports detection of age and emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "print(prompt_caption_gen)   \n",
    "analysis = DeepFace.analyze(img_path, actions = [\"gender\", \"race\",\"emotion\"],enforce_detection=False)\n",
    "print(analysis)\n",
    "tags_seen=[]\n",
    "for person in analysis:\n",
    "    if person[\"dominant_gender\"] not in tags_seen and person[\"dominant_race\"] not in tags_seen : \n",
    "        prompt_caption_gen += person[\"dominant_race\"] + \" \" + person[\"dominant_gender\"] + \" \" \n",
    "        tags_seen.append(person[\"dominant_race\"])\n",
    "        tags_seen.append(person[\"dominant_gender\"])\n",
    "\n",
    "    \n",
    "print(prompt_caption_gen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from promptcap import PromptCap\n",
    "\n",
    "model = PromptCap(\"tifa-benchmark/promptcap-coco-vqa\")  # also support OFA checkpoints. e.g. \"OFA-Sys/ofa-large\"\n",
    "\n",
    "image = \"curdir/nocaption.jpg\"\n",
    "caption_generated=model.caption(prompt_caption_gen, image)\n",
    "print(caption_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Similarity using semantic analysis\n",
    "\n",
    "Here, sentence similarity is used as the measure to determine whether an image is a meme or not. The two text fields are:\n",
    "\n",
    "1. The given text in the image.\n",
    "2. Caption generated based on the images, generated with the context of races and objects within the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "vector=model.encode([given_text,caption_generated])\n",
    "\n",
    "print(cosine_similarity([vector[0]],vector[1:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
